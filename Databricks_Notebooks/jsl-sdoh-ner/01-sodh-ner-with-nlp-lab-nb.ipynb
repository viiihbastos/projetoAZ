{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6979602-1148-46c8-a421-1fa9842781e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Annotating Data & Training Custom NER model for Social Determinants of Health (SDOH) Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7fe372d-7fae-4db6-8d1f-f6f1e69d76e3",
     "showTitle": false,
     "title": ""
    },
    "id": "zJvEsNlS0B7E"
   },
   "source": [
    "In this tutorial, we'll see how we can easily pre-annotate data using pre-defined vocabulary / key-word matching, and upload them as pre-annotations to NLP Lab.\n",
    "\n",
    "We'll be using the NLP Lab module to create, configure, export and import projects with minimal code (optional).\n",
    "\n",
    "Note: The NLP Lab module is available in Spark NLP for Healthcare 4.2.2+.\n",
    "\n",
    "## Following are the main steps in this exercise:\n",
    "\n",
    "### 1. Using string matching and existing off-the-shelf vocabularies, create a simple pipeline to get rudimentary results.\n",
    "### 2. Upload the initial results to NLP Lab, annotate, and download annotations(optinal, you can skip this part and use annotated data to continue).\n",
    "### 3. Train an NER model on the annotated data to achieve better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "789e7875-16d9-45f5-b27e-903ad3aa9895",
     "showTitle": false,
     "title": ""
    },
    "id": "cyD4sXG-0B7R"
   },
   "source": [
    "# 0. Initial configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef397e34-aca0-4fc1-a645-8b49a6fce8d0",
     "showTitle": false,
     "title": ""
    },
    "id": "7Ziy5laJ0B7S",
    "outputId": "b472f8ca-2b0c-4849-eac0-c0438194ef8d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "import sparknlp\n",
    "import sparknlp_jsl\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.base import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from sparknlp.training import CoNLL\n",
    "\n",
    "print('sparknlp.version : ',sparknlp.version())\n",
    "print('sparknlp_jsl.version : ',sparknlp_jsl.version())\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "783554cc-1421-4e5c-b5a1-a942c62a179c",
     "showTitle": false,
     "title": ""
    },
    "id": "Xc7NJrOc0B7W"
   },
   "source": [
    "For this exercise, we define two different layers of solutions:\n",
    "1. Bronze: Text Matcher based rudimentary results - that will be uploaded in NLP Lab and refined.\n",
    "2. Silver: After annotating the documents properly in NLP Lab, train an NER model, get results\n",
    "\n",
    "Let's define these paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1515569-f81d-4a65-acc4-83c2d167237e",
     "showTitle": false,
     "title": ""
    },
    "id": "yVeybJOA0B7W",
    "outputId": "d80f44f7-f535-4ae6-d857-1041c87620f4"
   },
   "outputs": [],
   "source": [
    "delta_bronze_path='/FileStore/SDOH/data/delta/bronze/'\n",
    "dbutils.fs.mkdirs(delta_bronze_path)\n",
    "os.environ['delta_bronze_path']=f'/dbfs{delta_bronze_path}'\n",
    "\n",
    "delta_silver_path='/FileStore/SDOH/data/delta/silver/'\n",
    "dbutils.fs.mkdirs(delta_silver_path)\n",
    "os.environ['delta_silver_path']=f'/dbfs{delta_silver_path}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "166c6483-f827-4c53-9624-0948f98125f2",
     "showTitle": false,
     "title": ""
    },
    "id": "1Uc0PLvs0B7X"
   },
   "source": [
    "# 1. Using string matching and existing off-the-shelf vocabularies, create a simple pipeline to get rudimentary results\n",
    "\n",
    "First, we'll rely on existing vocabularies comprising of key words e.g: \"unstable housing\", \"lack of insurance\", \"substance abusers\", etc to get preliminary results.\n",
    "\n",
    "The vocbulary set in this exercise is generated using JSL SDOH Internal Project data and can be obtained from https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/databricks/python/healthcare_case_studies/data/sdoh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7730b70d-5a62-41e6-b6b6-619e3d119b8e",
     "showTitle": false,
     "title": ""
    },
    "id": "q6N9mE4s0B7Y"
   },
   "source": [
    "**About the dataset:** The dataset is a text data that includes medical patient files with SDOH information. It is generated artificially using ChatGPT 3.5. This data can be downloaded from https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/databricks/python/healthcare_case_studies/data/sdoh/sdoh_sample.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3956fa71-6e6f-4542-bcf0-bb5e7e0f0a03",
     "showTitle": false,
     "title": ""
    },
    "id": "waIPuheM0B7c"
   },
   "source": [
    "## 1.1. Download resources and explore vocabulary\n",
    "\n",
    "The below list is the selected labels of the SDOH dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ed990d3-1914-4d5a-b008-b51ae977689d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "label_list = [\n",
    "\"Housing\",\n",
    "\"Substance_Use\",\n",
    "\"Insurance_Status\",\n",
    "\"Social_Exclusion\",\n",
    "\"Violence_Or_Abuse\",\n",
    "\"Spiritual_Beliefs\",\n",
    "\"Financial_Status\"\n",
    "] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f76c945-9a01-4b19-a6a2-bb4658e16f6e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Download vocabulary list for each label from JSL github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "581208e8-c0ff-4eb6-9825-4be7e5be337a",
     "showTitle": false,
     "title": ""
    },
    "id": "6-D6BxcA0B7d",
    "outputId": "822dc0ff-e588-4672-e64a-d008fe0f75ea"
   },
   "outputs": [],
   "source": [
    "\n",
    "%sh\n",
    "url_base=\"https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/databricks/python/healthcare_case_studies/data/sdoh/\"\n",
    "label_list=(\n",
    "    \"Housing\"\n",
    "    \"Substance_Use\"\n",
    "    \"Insurance_Status\"\n",
    "    \"Social_Exclusion\"\n",
    "    \"Violence_Or_Abuse\"\n",
    "    \"Spiritual_Beliefs\"\n",
    "    \"Financial_Status\"\n",
    ")\n",
    "\n",
    "cd $delta_bronze_path\n",
    "for label in \"${label_list[@]}\"; do\n",
    "    file_url=\"${url_base}${label}_word_list.txt\"\n",
    "    wget \"$file_url\"\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "895e99ae-a220-4f24-88d1-b8770ad94b4f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Checking the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "749608aa-aa8c-4c0a-9cd1-e9e51b57fb99",
     "showTitle": false,
     "title": ""
    },
    "id": "Kk7Yt3fd0B7d",
    "outputId": "57546eb6-b71b-4f9d-e51f-9bad55b61f86"
   },
   "outputs": [],
   "source": [
    "data_vocab_path = f\"{delta_bronze_path}\"\n",
    "dbutils.fs.ls(data_vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07b4143-8a8d-479e-a51a-54df5d2ef587",
     "showTitle": false,
     "title": ""
    },
    "id": "JGJizua60B7e"
   },
   "source": [
    "Taking a look at one of the vocabulary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a071da2e-96dc-4e6c-8f78-0abe574b5ee7",
     "showTitle": false,
     "title": ""
    },
    "id": "Cpn4QoDX0B7e",
    "outputId": "cba87161-f6c1-464e-aeef-cbbd3cb7b231"
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "head /dbfs/FileStore/SDOH/data/delta/bronze/Housing_word_list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6912c78d-df51-449a-8409-45c7d44bbcc7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we will download SDOH text data. This text data includes medical patient files including SDOH information. We will first pre-annotate them, then annotate and finally train a NER model.\n",
    "\n",
    "Let's define original text data folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ee99289-3d2f-4f44-9450-5acfb0039162",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "original_data_path='/FileStore/SDOH/data/delta/original/'\n",
    "dbutils.fs.mkdirs(original_data_path)\n",
    "os.environ['original_data_path']=f'/dbfs{original_data_path}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "102f5c72-9de8-42a8-b774-8a9051f3f01b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now dowload the SDOH text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22dd9fb1-8adb-4c14-8037-d5c83e6e88e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "cd $original_data_path\n",
    "wget https://github.com/JohnSnowLabs/spark-nlp-workshop/raw/master/databricks/python/healthcare_case_studies/data/sdoh/sdoh_sample.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de9f61b7-9094-4163-a672-3d70d7af9e1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Check the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7b59a7a-e681-4d33-b107-92af0abef460",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "head /dbfs/FileStore/SDOH/data/delta/original/sdoh_sample.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bc250ae-a46b-4f99-9596-45c639c0e38c",
     "showTitle": false,
     "title": ""
    },
    "id": "nngaSJkZ0B7f"
   },
   "source": [
    "## 1.2. Creating a Spark NLP pipeline using textmatchers to find entities in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eec5ba3-613e-4360-bc59-1620753f68e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Bleow pipeline is used for extracting vocabulary based entities. We used seperate TextMatcher annotator for each label, then merged all TextMatcher outputs at the end of the pipeline using ChunkMergeApproach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "016838d9-b23f-427f-a4ad-a5854522367a",
     "showTitle": false,
     "title": ""
    },
    "id": "9Lnef_LZ0B7f",
    "outputId": "31685894-51fc-4ba3-cb96-909113412eb9"
   },
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "sentenceDetector = SentenceDetector()\\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "  .setInputCols(\"sentence\")\\\n",
    "  .setOutputCol(\"token\")\n",
    "\n",
    "# get a annotator dictionay for all label from labels list\n",
    "text_matcher_dict={}\n",
    "for label in label_list:\n",
    "    text_matcher_dict[label] = TextMatcher().setInputCols(\"sentence\",\"token\").setOutputCol(label).setEntityValue(label)\\\n",
    "    .setEntities(data_vocab_path+f\"{label}_word_list.txt\").setCaseSensitive(False).setMergeOverlapping(True)\\\n",
    "    .setBuildFromTokens(True)\n",
    "\n",
    "chunk_merger = ChunkMergeApproach()\\\n",
    "    .setInputCols(label_list)\\\n",
    "    .setOutputCol(\"all_chunks\")\\\n",
    "\n",
    "pipeline =  Pipeline(\n",
    "    stages=[\n",
    "        documentAssembler,\n",
    "        sentenceDetector,\n",
    "        tokenizer,\n",
    "        *text_matcher_dict.values(),\n",
    "        chunk_merger\n",
    "    ]\n",
    ")\n",
    "\n",
    "p_model = pipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\"))\n",
    "\n",
    "l_model = LightPipeline(p_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c33ffa1-a6de-4e66-b06e-0d0b8c934b61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's check our pipeline with a text input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7424c98-4793-41b6-ab08-07a927b4c5c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sparknlp_display import NerVisualizer\n",
    "\n",
    "result = l_model.fullAnnotate(\" a 55-year-old veteran who has been experiencing housing insecurity since losing his job as a factory worker. Despite his service to his country, Carlos has been unable to secure stable housing, and has been forced to rely on shelters and transitional housing programs. He has financial problems.\")\n",
    "\n",
    "displayHTML(NerVisualizer().display(result[0], 'all_chunks', return_html=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9496ec9-ef24-4359-8076-50527a1d2f09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9c7a3cc-7000-448a-b23f-e236104b4df6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now read all SDOH text data as spark data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b88df820-1448-4695-b20b-fbca620ce4e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"dbfs:/FileStore/SDOH/data/delta/original/sdoh_sample.csv\", sep=',', multiLine=True, header=True).withColumnRenamed('_c0',\"text\")\n",
    "print(data.count())\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17c0695f-de6d-4f53-9fbe-85fa520c3724",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Transform dataframe using pre-annatation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50dd3245-7187-47c6-bcf3-fd1fcf02d361",
     "showTitle": false,
     "title": ""
    },
    "id": "fuFSXT4n0B7g",
    "outputId": "0d3eebdf-12af-498f-cf45-c954b6a45a6c"
   },
   "outputs": [],
   "source": [
    "results = p_model.transform(data).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b68235f-ffd0-405a-9105-085970836af1",
     "showTitle": false,
     "title": ""
    },
    "id": "ISFSo7zM0B7h"
   },
   "source": [
    "## 1.3. Analyzing Results\n",
    "\n",
    "Even the basic text matching approach works in general, but has the following challenges:\n",
    "1. Lack of context, leading to too many false positives.\n",
    "2. Less meaningful and incomplete chunks.\n",
    "\n",
    "Let's check some of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7647be61-7216-4ad8-8a10-a69b76e6440c",
     "showTitle": false,
     "title": ""
    },
    "id": "zvRoS0o20B7h",
    "outputId": "a173c14f-f0cb-4aa8-edd0-5e65a59d6301"
   },
   "outputs": [],
   "source": [
    "from sparknlp_display import NerVisualizer\n",
    "\n",
    "displayHTML(NerVisualizer().display(results[55], 'all_chunks', return_html=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcccd4e7-2c8d-4d98-a85c-ec84d1342092",
     "showTitle": false,
     "title": ""
    },
    "id": "tEQaHQBZ0B7i",
    "outputId": "1f90c33f-ab47-415f-93aa-9e2cb71a9b7c"
   },
   "outputs": [],
   "source": [
    "# Checking on a piece of text\n",
    "text = \"\"\"a 55-year-old veteran who has been experiencing housing insecurity since losing his job as a factory worker. Despite his service to his country, Carlos has been unable to secure stable housing, and has been forced to rely on shelters and transitional housing programs. He has financial problems. He has a history of drug abuse \"\"\"\n",
    "\n",
    "results_single = l_model.fullAnnotate(text)[0]\n",
    "\n",
    "from sparknlp_display import NerVisualizer\n",
    "\n",
    "displayHTML(NerVisualizer().display(results_single, 'all_chunks', return_html=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0752ceaf-c766-4214-8052-888d123a3f75",
     "showTitle": false,
     "title": ""
    },
    "id": "9EZX-HqM0B7i"
   },
   "source": [
    "# 2. Upload pre-annotations to the NLP Lab\n",
    "\n",
    "Now, we can use these results as pre-annotations and upload them to the NLP Lab. Pre-annotations help reduce manual annotation time as the annotator does not need to annotate everything, but rather make corrections.\n",
    "\n",
    "For this exercise, we are using John Snow Lab's NLP Lab tool.\n",
    "\n",
    "The NLP Lab is a stand-alone web interface designed to be used and installed inside any organization's environment to protect data privacy. It can be easily installed on a single VM.\n",
    "\n",
    "More details and instructions can be found here: https://nlp.johnsnowlabs.com/docs/en/alab/install#aws-marketplace. \n",
    "\n",
    "While the tasks and pre-annotations can be uploaded directly via web interface as well, we are leveraging the API module for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7922d5-5ad5-4875-8039-6c4eb57b88bb",
     "showTitle": false,
     "title": ""
    },
    "id": "lk7f1xPM0B7j"
   },
   "source": [
    "## 2.1. Generate Pre-annotations using the Annotation Lab Module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cceb8fa8-49c7-4ff7-a7c7-7a0cf705845a",
     "showTitle": false,
     "title": ""
    },
    "id": "JSDyHSBY0B7j"
   },
   "source": [
    "Initialize Alab module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "523783e3-6eae-4b66-bcbb-c4dac62f029b",
     "showTitle": false,
     "title": ""
    },
    "id": "yf6a8b0O0B7j",
    "outputId": "e87996df-6484-454e-a80c-de00f5d1a4b9"
   },
   "outputs": [],
   "source": [
    "from sparknlp_jsl.alab import AnnotationLab\n",
    "\n",
    "alab = AnnotationLab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cfc5e7f-af63-4052-82b6-19d0a0c16741",
     "showTitle": false,
     "title": ""
    },
    "id": "1-47i7Lc0B7k",
    "outputId": "a563253b-dfe7-4a03-b6c7-b655c47a531f"
   },
   "outputs": [],
   "source": [
    "# NOTE: \"all_results\" is the result of the pipeline after running on sample docs.\n",
    "pre_annotations, summary = alab.generate_preannotations(all_results = results, document_column = 'document', ner_columns = ['all_chunks'])\n",
    "pre_annotations[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "327bc275-71d0-43fc-bb83-bac8a47c95d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**What is the summary?** \n",
    "\n",
    "While a user would know all the entities coming out of the NLP pipeline, listing all of them manually is laborious.\n",
    "The summary object helps identify how many types of entities, assertions, and relations are present.\n",
    "This saves the user from listing all labels individually; we'll use this object while setting project configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88189e33-4226-4f8e-8b78-2d3753d43410",
     "showTitle": false,
     "title": ""
    },
    "id": "4Tsaszjs0B7l",
    "outputId": "6bd53d8c-4402-484c-8c8a-21294b712d50"
   },
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d569568-4ebb-48be-8efc-d336a8148324",
     "showTitle": false,
     "title": ""
    },
    "id": "XdUo3uqN0B7l"
   },
   "source": [
    "## 2.2. Set NLP Lab credentials and Create a New Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a3b654a-3448-493d-9003-0198ddfa3a39",
     "showTitle": false,
     "title": ""
    },
    "id": "fiEhx0OS0B7l"
   },
   "source": [
    "<font color=#FF0000>**Note: If you don't have credentials for the NLP Lab, we have provided the annotations for ~300 tasks. Jump to section 3.2 to directly download the exported JSON file, and start training**</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3732d9cb-42b9-4313-baea-084f8504ebd3",
     "showTitle": false,
     "title": ""
    },
    "id": "aVJudqev0B7o",
    "outputId": "6eccf95f-d204-42d3-d66e-29b61df6a312"
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Set Credentials\n",
    "# username=''\n",
    "# password=''\n",
    "# client_secret=\"\" # see https://nlp.johnsnowlabs.com/docs/en/alab/api#get-client-secret\n",
    "# annotationlab_url=\"\" # your alab instance URL (could even be internal URL if deployed on-prem).\n",
    "\n",
    "# alab.set_credentials(\n",
    "\n",
    "#   # required: username\n",
    "#   username=username,\n",
    "\n",
    "#   # required: password\n",
    "#   password=password,\n",
    "\n",
    "#   # required: secret for you alab instance (every alab installation has a different secret)\n",
    "#   client_secret=client_secret, \n",
    "\n",
    "#   # required: http(s) url for you NLP Lab\n",
    "#   annotationlab_url=annotationlab_url\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "113a83af-b76a-4145-8273-7a9b76d9fb87",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create a new project \"sdoh\" at NLP Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80470e8d-70e5-4688-8e04-baccef66e248",
     "showTitle": false,
     "title": ""
    },
    "id": "v_E5mbKP0B7p",
    "outputId": "804bf492-0022-44d9-be23-9b3800d35602"
   },
   "outputs": [],
   "source": [
    "# alab.create_project('sdoh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2efa859-874e-475d-91d8-f4133a562485",
     "showTitle": false,
     "title": ""
    },
    "id": "mkY1rQrN0B7p"
   },
   "source": [
    "## 2.3. Set project configuration (NER label tags, assertion classes, and relation tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f57aa80-9f57-48f8-9321-88522752dd7e",
     "showTitle": false,
     "title": ""
    },
    "id": "uatLG0nu0B7p",
    "outputId": "971d4cab-be64-497b-f4f5-d15a4292f5b5"
   },
   "outputs": [],
   "source": [
    "# # set configuration\n",
    "\n",
    "# ## either manually define labels:\n",
    "# # alab.set_project_config(\n",
    "# #   project_name = 'suicide_detection',\n",
    "# #   ner_labels = [\"Housing\", \"Substance_Use\", \"Insurance_Status\", \"Social_Exclusion\", \"Violence_Or_Abuse\", \"Spiritual_Beliefs\", \"Financial_Status\"]\n",
    "# # )\n",
    "\n",
    "# # OR use the summary object which already has all details\n",
    "\n",
    "# alab.set_project_config(\n",
    "#   project_name = 'sdoh',\n",
    "#   ner_labels = summary['ner_labels'],\n",
    "#   assertion_labels = summary['assertion_labels'],\n",
    "#   relations_labels = summary['re_labels']\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f37418e-37bd-4e65-a1db-a37980d4992f",
     "showTitle": false,
     "title": ""
    },
    "id": "FFmbBrIg0B7q"
   },
   "source": [
    "## 2.4. Upload pre-annotations to the newly created project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6384d9e-dd4e-4827-88c7-b52751aa70e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can upload all the tasks and annotate. For demo purpose, we are only uploading 5 tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3fec6b9-d631-4c0c-8c34-198655588177",
     "showTitle": false,
     "title": ""
    },
    "id": "kHoDNwAu0B7q",
    "outputId": "2ec07b6b-b711-44b3-b0c2-70ecb5429b8c"
   },
   "outputs": [],
   "source": [
    "# # Upload documents to Alab\n",
    "\n",
    "# alab.upload_preannotations(\n",
    "#   project_name = 'sdoh',\n",
    "#   preannotations = pre_annotations[:5]) # testing with 5 annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a9e2580-e6d2-42a5-a9e7-4bda4ab1b2ce",
     "showTitle": false,
     "title": ""
    },
    "id": "BPpQLExA0B7q"
   },
   "source": [
    "## 2.5. Annotate documents on NLP Lab and make necessary corrections\n",
    "\n",
    "**2.5.1 The first step for annotations is developing, and adhering to some guidelines, which are crucial for controlling the flow of annotations and avoiding confusion between entitiy types.**\n",
    "\n",
    ">**An example Annotation Guideline (AG) is available [here](http://www.universalner.org/guidelines/).**\n",
    "\n",
    "**2.5.2 Once annotation guidelines have been finalized, annotations can be started.**\n",
    "\n",
    "**Since we have already uploaded pre-annotations to the NLP Lab, we can get started.**\n",
    "\n",
    "1.       Go to the Projects -> sdoh -> tasks\n",
    "\n",
    "2.       Select the first task\n",
    "\n",
    "3.       Click Edit -> select NER type -> select corresponding text, as defined in Annotation Guidelines.\n",
    "\n",
    "4.       Click Save -> Submit.\n",
    "\n",
    "5.       Go to the next task, until all tasks are completed.\n",
    "\n",
    "In the tasks overview, you should see all 5 tasks submitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1df4cf06-f7fc-4fe0-a46c-2754bbaa2e60",
     "showTitle": false,
     "title": ""
    },
    "id": "RFqelnPI0B7z"
   },
   "source": [
    "# 3. Train NER model\n",
    "Now, we can train an NER model using the annotations performed on the NLP Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b39ed746-af57-48ab-ba57-4d0df9ffaa30",
     "showTitle": false,
     "title": ""
    },
    "id": "DRBIWpDR0B70"
   },
   "source": [
    "## 3.1. Import the annotations from NLP Lab and save them as a JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef7212a0-6570-4149-823b-472316111598",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First We will download annotations info from NLP Lab project as a JSON file. This JSON file includes url of the annotations zip file. Then we will extract it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa782fb0-020c-4af0-aedd-263d529c3144",
     "showTitle": false,
     "title": ""
    },
    "id": "vK-YyMW60B70",
    "outputId": "c9f18045-0e10-4124-aa73-71182bff7d7c"
   },
   "outputs": [],
   "source": [
    "# exported_json = alab.get_annotations(\n",
    "#                 project_name = 'sdoh', \n",
    "#                 output_name='result',\n",
    "#                 save_dir=f\"/dbfs{delta_silver_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b3a282-ef39-4be2-9284-e9db60eee9b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\"exported_json\" includes download link for the annotation zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb5868ae-297f-4137-a203-4b583854a1e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# exported_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86b43ea3-5e9c-40c0-8785-669c7fed62ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Save download_link and filename asenvironment variables to be used in command shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff631d2d-6e71-4b17-aa84-406395f36031",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# os.environ['download_link'] = exported_json[\"download_link\"]\n",
    "# os.environ['export_filename']  = exported_json[\"download_link\"].split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93120f86-5b63-4655-a6c3-a430dccea37b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will dowload and unzip the zip file mentioned in the exported_json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c67833d-1dbf-45b9-bb74-2e60264ee9cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sh\n",
    "# cd $delta_silver_path\n",
    "# wget -q $download_link\n",
    "# unzip -o $export_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dfd4e9e-70a4-4e2e-8637-dd6d2c6dc6f5",
     "showTitle": false,
     "title": ""
    },
    "id": "_LCuI8_N0B70",
    "outputId": "b9b7e701-052d-4ccc-f9a3-0de20c77d90a"
   },
   "outputs": [],
   "source": [
    "# print (delta_silver_path)\n",
    "# dbutils.fs.ls(delta_silver_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b3eb7e6-5b18-44b8-8374-ce19358afc5f",
     "showTitle": false,
     "title": ""
    },
    "id": "zx9pPhL_0B71"
   },
   "source": [
    "## 3.2. Convert the JSON file to CoNLL format for training an NER model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee05a3ff-c173-41b2-96d5-be02a915ce93",
     "showTitle": false,
     "title": ""
    },
    "id": "zFIGpU0I0B71"
   },
   "source": [
    "<font color=#FF0000>**Note: For demo purpose, we are downloading the annotated tasks. This is the same data as above NLP Lab annotations**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f160f9e-261a-4be2-974d-66b53a48560b",
     "showTitle": false,
     "title": ""
    },
    "id": "o9bYxf9k0B72"
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "cd $delta_silver_path\n",
    "wget https://github.com/JohnSnowLabs/spark-nlp-workshop/raw/master/databricks/python/healthcare_case_studies/data/sdoh/sdoh_sample_export.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a51417f2-0f7b-47f7-b4cb-947244a2feca",
     "showTitle": false,
     "title": ""
    },
    "id": "-xX-srdM0B73"
   },
   "outputs": [],
   "source": [
    "print (delta_silver_path)\n",
    "dbutils.fs.ls(delta_silver_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9ae5633-8dc5-4eca-9997-7506c27102a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Initialize AnnotationLab modeule and create conll data from the annotation JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00f0e706-afce-4f21-8838-684842c95558",
     "showTitle": false,
     "title": ""
    },
    "id": "0QnOO3-w0B73",
    "outputId": "8a972d17-39d3-44d7-99a0-12fc6dd893d7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sparknlp_jsl.alab import AnnotationLab\n",
    "\n",
    "alab = AnnotationLab()\n",
    "\n",
    "alab.get_conll_data(spark, f\"/dbfs{delta_silver_path}sdoh_sample_export.json\", output_name='conll_demo', save_dir=f\"/dbfs{delta_silver_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c577d36d-b5c7-4776-af9a-2652b6fa4733",
     "showTitle": false,
     "title": ""
    },
    "id": "33QJqEjp0B73",
    "outputId": "7bab55d0-bcc0-4dcd-fdec-8fe26b4bc0b9"
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(delta_silver_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ffc8fa8-3f9b-441b-8198-3b06415d22b6",
     "showTitle": false,
     "title": ""
    },
    "id": "pCFyUe5E0B74"
   },
   "source": [
    "## 3.3. Train NER Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf475498-5740-4ebe-9b4e-7b1d0ef20296",
     "showTitle": false,
     "title": ""
    },
    "id": "XjWjNegS0B74"
   },
   "source": [
    "Load conll data to be used for NER trainig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55c27210-76fa-421d-a269-231a785541a6",
     "showTitle": false,
     "title": ""
    },
    "id": "-GvXsx8e0B74",
    "outputId": "f1d87983-0c6d-4ae8-978f-9692b781ff18"
   },
   "outputs": [],
   "source": [
    "conll_data = CoNLL().readDataset(spark, f\"{delta_silver_path}conll_demo.conll\")\n",
    "conll_data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c395d6a-4758-4dfc-a17a-32894fc3ad84",
     "showTitle": false,
     "title": ""
    },
    "id": "eseurTpU0B74"
   },
   "source": [
    "Look at label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a048991a-c514-478f-a0c4-539e028ffeaa",
     "showTitle": false,
     "title": ""
    },
    "id": "yRJHwRFp0B74",
    "outputId": "f29999b5-95a3-4846-a2a9-70b77e132e32"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "conll_data.select(F.explode(F.arrays_zip(conll_data.token.result,\n",
    "                                         conll_data.label.result)).alias(\"cols\")) \\\n",
    "          .select(F.expr(\"cols['0']\").alias(\"token\"),\n",
    "                  F.expr(\"cols['1']\").alias(\"ground_truth\"))\\\n",
    "          .groupBy('ground_truth')\\\n",
    "          .count()\\\n",
    "          .orderBy('count', ascending=False)\\\n",
    "          .show(100,truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1c87bd8-d675-4b3b-ad48-00bd16d12f2d",
     "showTitle": false,
     "title": ""
    },
    "id": "lWEDQflo0B75"
   },
   "source": [
    "Select Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc0ab4b8-6a4d-4046-b0b9-d9964b58d5f2",
     "showTitle": false,
     "title": ""
    },
    "id": "cTCjbMtA0B75",
    "outputId": "c8dbd845-edb1-412e-e1df-6fb7372d8e32"
   },
   "outputs": [],
   "source": [
    "clinical_embeddings = WordEmbeddingsModel.pretrained('embeddings_clinical', \"en\", \"clinical/models\")\\\n",
    "    .setInputCols([\"sentence\", \"token\"])\\\n",
    "    .setOutputCol(\"embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab8a6789-4c42-48e4-8cda-fb1960705c52",
     "showTitle": false,
     "title": ""
    },
    "id": "medW_4Gj0B75"
   },
   "source": [
    "Graph Builder to automatically generate a TensorFlow graph for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcaafe09-9de6-478b-b434-f3df3ca82a23",
     "showTitle": false,
     "title": ""
    },
    "id": "xC8ci50l0B75",
    "outputId": "162d94d1-8c64-4004-cfe4-6ff817cb082a"
   },
   "outputs": [],
   "source": [
    "graph_folder_path = \"/dbfs/ner/medical_ner_graphs\"\n",
    "\n",
    "ner_graph_builder = TFGraphBuilder()\\\n",
    "    .setModelName(\"ner_dl\")\\\n",
    "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "    .setLabelColumn(\"label\")\\\n",
    "    .setGraphFolder(graph_folder_path)\\\n",
    "    .setGraphFile(\"auto\")\\\n",
    "    .setHiddenUnitsNumber(20)\\\n",
    "    .setIsLicensed(True) # False -> if you want to use TFGraphBuilder with NerDLApproach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52086aea-8108-4892-ab33-03d77bca763d",
     "showTitle": false,
     "title": ""
    },
    "id": "FazPN5860B75"
   },
   "source": [
    "Below is the NER Model training Aproach. We define all hyper-parameters within this step. At the end we define our trainig pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d4d2957-2ec1-46ba-aec1-5fed12fcecfd",
     "showTitle": false,
     "title": ""
    },
    "id": "_JKlnKv90B76",
    "outputId": "3d1b673f-4ff5-440a-c575-387f6bd446a5"
   },
   "outputs": [],
   "source": [
    "nerTagger = MedicalNerApproach()\\\n",
    "  .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
    "  .setLabelColumn(\"label\")\\\n",
    "  .setOutputCol(\"ner\")\\\n",
    "  .setMaxEpochs(21)\\\n",
    "  .setBatchSize(32)\\\n",
    "  .setRandomSeed(0)\\\n",
    "  .setVerbose(1)\\\n",
    "  .setLr(0.001)\\\n",
    "  .setEvaluationLogExtended(True) \\\n",
    "  .setEnableOutputLogs(True)\\\n",
    "  .setOutputLogsPath('dbfs:/ner/ner_logs')\\\n",
    "  .setUseBestModel(True)\\\n",
    "  .setGraphFolder('dbfs:/ner/medical_ner_graphs')\\\n",
    "  .setLogPrefix(\"sdoh_demo\")\\\n",
    "  .setValidationSplit(0.2)\n",
    "  # .setEnableMemoryOptimizer(True) #>> If you have limited memory and a large conll file, you can set this True to train batch by batch       \n",
    "\n",
    "ner_pipeline = Pipeline(stages=[\n",
    "          clinical_embeddings,\n",
    "          ner_graph_builder,\n",
    "          nerTagger\n",
    " ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "091cb3a6-1b61-4a49-9968-0d4cf855e726",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Start NER training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "954d1501-ae2c-4c52-a14e-c75a7e8a404f",
     "showTitle": false,
     "title": ""
    },
    "id": "AfNUxcFu0B76",
    "outputId": "4de81f34-e00b-4b55-c9d0-ac8f418cb852"
   },
   "outputs": [],
   "source": [
    "model = ner_pipeline.fit(conll_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2c621d9-cf05-4458-af9f-7ddcf67651e3",
     "showTitle": false,
     "title": ""
    },
    "id": "tfIHS1CB0B76"
   },
   "source": [
    "Check logs for loss and scores during batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f0e0fb8-3d77-4ea8-ac8e-3c41f6634e20",
     "showTitle": false,
     "title": ""
    },
    "id": "R7u08lL80B76",
    "outputId": "2aaad6be-94e1-4cde-f928-3ede1799724d"
   },
   "outputs": [],
   "source": [
    "ls -l /dbfs/ner/ner_logs/sdoh_demo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dd98d83-c75e-4fe4-8f59-f14b7a2491e0",
     "showTitle": false,
     "title": ""
    },
    "id": "Lv9afpUM0B77",
    "outputId": "741f7610-4265-4b7c-bdf0-0f2426a3c49d"
   },
   "outputs": [],
   "source": [
    "sample_log_file_name = dbutils.fs.ls(\"/ner/ner_logs/\")[0].name\n",
    "with open(f'/dbfs/ner/ner_logs/{sample_log_file_name}', 'r') as f_:\n",
    "  lines = ''.join(f_)\n",
    "print (lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccc1233a-be6b-4454-9198-9edcf2d7b9d3",
     "showTitle": false,
     "title": ""
    },
    "id": "dQPcUosv0B77"
   },
   "source": [
    "## 3.4. Save the model to disk, load from the disk and test on the new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65d3ad3f-c021-422b-847c-81fb934da9ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Save the trained NER model at \"silver_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b320b811-6599-4748-8c21-0f0acbd547b2",
     "showTitle": false,
     "title": ""
    },
    "id": "0Jh_oVOS0B77",
    "outputId": "6c09da88-efad-4177-fea3-9883015fc46e"
   },
   "outputs": [],
   "source": [
    "model.stages[-1].write().overwrite().save(delta_silver_path+'ner_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b1ff7a4-ba15-488d-b123-2cac816190f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now build prediction pipeline using trained & saved NER model. We will load saved modwel from the \"silver_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51029d92-da5a-45ba-886f-b63a82c42a72",
     "showTitle": false,
     "title": ""
    },
    "id": "nObjzXZA0B77",
    "outputId": "0810bbe3-ef8f-4233-c8ff-9b4f4a037fed"
   },
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentenceDetector = SentenceDetectorDLModel().pretrained('sentence_detector_dl_healthcare', 'en', 'clinical/models')\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols([\"sentence\"])\\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "embeddings = WordEmbeddingsModel.pretrained('embeddings_clinical', 'en', 'clinical/models') \\\n",
    "    .setInputCols(\"sentence\", \"token\") \\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "#load the trained ner model\n",
    "ner_model =MedicalNerModel().load(delta_silver_path+'ner_model')\\\n",
    "    .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
    "    .setOutputCol(\"ner_tags\")\n",
    "\n",
    "#ner converter ner jsl enriched\n",
    "ner_chunk = NerConverterInternal()\\\n",
    "    .setInputCols(['sentence', 'token', 'ner_tags']) \\\n",
    "    .setOutputCol('ner_chunk')\n",
    "\n",
    "\n",
    "pipeline=Pipeline(stages=[\n",
    "    documentAssembler,\n",
    "    sentenceDetector,\n",
    "    tokenizer,\n",
    "    embeddings,\n",
    "    ner_model,\n",
    "    ner_chunk,\n",
    "])\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "model = pipeline.fit(empty_data)\n",
    "light_model = LightPipeline(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6372c002-5058-4a82-889c-247320358c99",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's check the new model on a piece of sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07ade4de-9cf2-4950-bb3b-bde4698b8ce5",
     "showTitle": false,
     "title": ""
    },
    "id": "Eb5TqRpn0B78",
    "outputId": "48dfa622-76a5-4934-98b8-68d296f97bba"
   },
   "outputs": [],
   "source": [
    "# Checking on a piece of text\n",
    "text = \"\"\"A 48-year-old woman presented at the community health clinic, highlighting a range of health concerns that were intricately connected to various social determinants of health.\n",
    "\n",
    "Her housing situation revealed instability, as she navigated temporary shelters due to financial constraints. This financial stress further impacted her ability to access adequate nutrition and consistent healthcare.\n",
    "\n",
    "The patient disclosed occasional substance use as a coping mechanism for her challenges, expressing a desire to explore healthier ways of managing stress.\n",
    "\n",
    "Having no health insurance amplified her worries about affording medical care and essential medications, adding to her emotional distress.\n",
    "\n",
    "Her social interactions were affected by feelings of exclusion arising from her housing instability and financial struggles. Additionally, the patient shared a history of emotional abuse, contributing to her overall stress levels.\n",
    "\n",
    "In contrast, the patient's spiritual beliefs as Christianity acted as a source of strength and emotional support, offering her solace during difficult times.\n",
    "\n",
    "The patient reported symptoms of anxiety and insomnia, warranting a comprehensive approach to her health management. Recommendations encompassed counseling for substance use, anxiety, and emotional support. Moreover, resources were provided to explore affordable healthcare services and potential insurance options.\"\"\"\n",
    "\n",
    "results_single = light_model.fullAnnotate(text)[0]\n",
    "\n",
    "from sparknlp_display import NerVisualizer\n",
    "\n",
    "displayHTML(NerVisualizer().display(results_single, 'ner_chunk', return_html=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74125617-98e4-4add-9f2b-431fe48550a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The model is trained against the `Housing, Substance_Use, Insurance_Status, Social_Exclusion, Violence_Or_Abuse, Spiritual_Beliefs, Financial_Status` and we extracted these entities from a patients file as seen above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbb46156-b4aa-430c-9a7c-d04f0a095cdf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## License\n",
    "\n",
    "Copyright / License info of the notebook. Copyright [2022] the Notebook Authors.  The source in this notebook is provided subject to the [Apache 2.0 License](https://spdx.org/licenses/Apache-2.0.html).  All included or referenced third party libraries are subject to the licenses set forth below.\n",
    "|Library Name|Library License|Library License URL|Library Source URL|\n",
    "| :-: | :-:| :-: | :-:|\n",
    "|Pandas |BSD 3-Clause License| <https://github.com/pandas-dev/pandas/blob/master/LICENSE> | <https://github.com/pandas-dev/pandas>|\n",
    "|Numpy |BSD 3-Clause License| <https://github.com/numpy/numpy/blob/main/LICENSE.txt> | <https://github.com/numpy/numpy>|\n",
    "|Apache Spark |Apache License 2.0| <https://github.com/apache/spark/blob/master/LICENSE> | <https://github.com/apache/spark/tree/master/python/pyspark>|\n",
    "|Requests|Apache License 2.0|<https://github.com/psf/requests/blob/main/LICENSE>|<https://github.com/psf/requests>|\n",
    "|Spark NLP Display|Apache License 2.0|<https://github.com/JohnSnowLabs/spark-nlp-display/blob/main/LICENSE>|<https://github.com/JohnSnowLabs/spark-nlp-display>|\n",
    "|Spark NLP |Apache License 2.0| <https://github.com/JohnSnowLabs/spark-nlp/blob/master/LICENSE> | <https://github.com/JohnSnowLabs/spark-nlp>|\n",
    "|Spark NLP for Healthcare|[Proprietary license - John Snow Labs Inc.](https://www.johnsnowlabs.com/spark-nlp-health/) |NA|NA|\n",
    "|Author|\n",
    "|-|\n",
    "|Databricks Inc.|\n",
    "|John Snow Labs Inc.|\n",
    "\n",
    "## Disclaimers\n",
    "\n",
    "Databricks Inc. (“Databricks”) does not dispense medical, diagnosis, or treatment advice. This Solution Accelerator (“tool”) is for informational purposes only and may not be used as a substitute for professional medical advice, treatment, or diagnosis. This tool may not be used within Databricks to process Protected Health Information (“PHI”) as defined in the Health Insurance Portability and Accountability Act of 1996, unless you have executed with Databricks a contract that allows for processing PHI, an accompanying Business Associate Agreement (BAA), and are running this notebook within a HIPAA Account.  Please note that if you run this notebook within Azure Databricks, your contract with Microsoft applies.\n",
    "\n",
    "The job configuration is written in the RUNME notebook in json format. The cost associated with running the accelerator is the user's responsibility."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2488707797227026,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "01-sodh-ner-with-nlp-lab-nb",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
